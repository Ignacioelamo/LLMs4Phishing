{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161ec15-b9c6-4655-8cac-1eefc3e32728",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade keybert\n",
    "!pip install sentence-transformers\n",
    "!pip install transformers accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f3756-7c52-4c92-b507-9231a46d6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cf4f4-5521-4aae-a7be-6d67b0923648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "generator = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a69802-b09b-42ff-ae88-47e9e18e4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"\"\"\n",
    "<s>[INST]\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else.\n",
    "[/INST] delivery, website, received, couple of days, still not received</s>\n",
    "\"\"\"\n",
    "\n",
    "keyword_prompt = \"\"\"\n",
    "[INST]\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "KeyBERT has generated the following candidate keywords:\n",
    "[CANDIDATES]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document and just one list\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = example_prompt + keyword_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec94e8-1a4b-432a-beea-cc0e2e699549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM\n",
    "\n",
    "llm = TextGeneration(generator, prompt=prompt_template)\n",
    "kw_model = KeyLLM(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aca248-c142-41e9-8d37-5136a22e35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\",\n",
    "    \"I received my package!\",\n",
    "    \"Meta released LLaMA's model weights to the research community under a noncommercial license.\",\n",
    "    \"My name is Juanje and I like dicks\",\n",
    "    \"I love viagra and I want hardsex\",\n",
    "]\n",
    "\n",
    "\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3d3c4-eebb-48f2-b7d2-d4978acf6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "kw_model = KeyBERT(llm=llm, model=embedding_model)\n",
    "\n",
    "keywords = kw_model.extract_keywords(documents, keyphrase_ngram_range=(1, 3), threshold=0.9, diversity=0.7, use_mmr=True, top_n=7)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f2b3c-c29d-44bf-b364-bfe34f92d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23213511-c7b5-40e0-960d-784f28eed2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM, KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para cargar emails de forma eficiente\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, start_idx=0):\n",
    "        self.df = dataframe.iloc[start_idx:].reset_index(drop=True)\n",
    "        self.original_indices = dataframe.iloc[start_idx:].index.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'index': self.original_indices[idx],\n",
    "            'body': str(row['body']) if pd.notna(row['body']) else \"\",\n",
    "            'subject': str(row['subject']) if pd.notna(row['subject']) else \"\"\n",
    "        }\n",
    "\n",
    "class KeywordExtractor:\n",
    "    \"\"\"Extractor de keywords con sistema de checkpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, df, checkpoint_file='keyword_extraction_checkpoint.pkl', \n",
    "                 results_file='keyword_results_temp.csv'):\n",
    "        self.df = df.copy()\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.results_file = results_file\n",
    "        self.model = None\n",
    "        self.generator = None\n",
    "        self.kw_model = None\n",
    "        \n",
    "        # Inicializar columna de keywords si no existe\n",
    "        if 'keywords' not in self.df.columns:\n",
    "            self.df['keywords'] = None\n",
    "            \n",
    "    def setup_model(self):\n",
    "        \"\"\"Configurar el modelo y pipeline\"\"\"\n",
    "        logger.info(\"Configurando modelo...\")\n",
    "        \n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            model=self.model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            max_new_tokens=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Configurar prompt template\n",
    "        example_prompt = \"\"\"\n",
    "<s>[INST]\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else.\n",
    "[/INST] delivery, website, received, couple of days, still not received</s>\n",
    "\"\"\"\n",
    "\n",
    "        keyword_prompt = \"\"\"\n",
    "[INST]\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "KeyBERT has generated the following candidate keywords:\n",
    "[CANDIDATES]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document and just one list\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "        prompt_template = example_prompt + keyword_prompt\n",
    "\n",
    "        # Configurar KeyBERT con LLM\n",
    "        llm = TextGeneration(self.generator, prompt=prompt_template)\n",
    "        \n",
    "        # Modelo de embeddings\n",
    "        embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "        \n",
    "        self.kw_model = KeyBERT(llm=llm, model=embedding_model)\n",
    "        \n",
    "        logger.info(\"Modelo configurado correctamente\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Cargar checkpoint si existe\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'rb') as f:\n",
    "                    checkpoint = pickle.load(f)\n",
    "                \n",
    "                processed_indices = checkpoint.get('processed_indices', set())\n",
    "                \n",
    "                # Cargar resultados parciales si existen\n",
    "                if os.path.exists(self.results_file):\n",
    "                    temp_results = pd.read_csv(self.results_file)\n",
    "                    for _, row in temp_results.iterrows():\n",
    "                        if row['index'] in self.df.index:\n",
    "                            self.df.loc[row['index'], 'keywords'] = row['keywords']\n",
    "                \n",
    "                logger.info(f\"Checkpoint cargado: {len(processed_indices)} emails ya procesados\")\n",
    "                return processed_indices\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error cargando checkpoint: {e}\")\n",
    "                return set()\n",
    "        \n",
    "        return set()\n",
    "    \n",
    "    def save_checkpoint(self, processed_indices):\n",
    "        \"\"\"Guardar checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'processed_indices': processed_indices,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_processed': len(processed_indices)\n",
    "            }\n",
    "            \n",
    "            with open(self.checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "            # Guardar resultados parciales\n",
    "            processed_df = self.df[self.df['keywords'].notna()].copy()\n",
    "            processed_df = processed_df.reset_index()\n",
    "            processed_df[['index', 'keywords']].to_csv(self.results_file, index=False)\n",
    "            \n",
    "            logger.info(f\"Checkpoint guardado: {len(processed_indices)} emails procesados\")\n",
    "            self.df.to_pickle(\"emails_with_keywords.pkl\")\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando checkpoint: {e}\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Limpiar y normalizar texto\"\"\"\n",
    "\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())  # Normalizar espacios\n",
    "        # Remover caracteres especiales y n√∫meros, mantener solo letras y espacios\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "        # Eliminar URLs y direcciones de correo\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S*@\\S*\\s?', '', text) # Eliminar emails\n",
    "        re.sub(r'https?://\\S+', ' ', text) # Por si queda alg√∫n http o https suelto\n",
    "    \n",
    "        # Eliminar caracteres especiales y puntuaci√≥n (dejando solo letras y n√∫meros por ahora)\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "        # 5. Eliminar n√∫meros sueltos y c√≥digos alfanum√©ricos largos\n",
    "        text = re.sub(r'\\b\\d+\\b', '', text) # Eliminar n√∫meros sueltos\n",
    "        # Eliminar palabras que son predominantemente n√∫meros o c√≥digos (ej. >4 chars y tiene un n√∫mero)\n",
    "        text = \" \".join([word for word in text.split() if not (len(word) > 3 and any(char.isdigit() for char in word) and not any(char.isalpha() for char in word.replace('.','')))]) # more specific for codes\n",
    "        text = re.sub(r'\\b[a-z]*\\d+[a-z\\d]*\\b', lambda m: '' if len(m.group(0)) > 5 and sum(c.isdigit() for c in m.group(0)) > sum(c.isalpha() for c in m.group(0)) / 2 else m.group(0), text)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def extract_keywords_batch(self, texts):\n",
    "        \"\"\"Extraer keywords de un lote de textos\"\"\"\n",
    "        if not texts:\n",
    "            return [\"\"] * len(texts)\n",
    "        \n",
    "        try:\n",
    "            # Filtrar textos v√°lidos\n",
    "            valid_texts = []\n",
    "            valid_indices = []\n",
    "            \n",
    "            for i, text in enumerate(texts):\n",
    "                cleaned = self.clean_text(text)\n",
    "                if cleaned:\n",
    "                    valid_texts.append(cleaned)\n",
    "                    valid_indices.append(i)\n",
    "            \n",
    "            if not valid_texts:\n",
    "                return [\"\"] * len(texts)\n",
    "            \n",
    "            # Extraer keywords en lote\n",
    "            keywords_results = self.kw_model.extract_keywords(\n",
    "                valid_texts, \n",
    "                keyphrase_ngram_range=(1, 3), \n",
    "                threshold=0.9, \n",
    "                diversity=0.7, \n",
    "                use_mmr=True, \n",
    "                top_n=5\n",
    "            )\n",
    "            \n",
    "            # Procesar resultados\n",
    "            results = [\"\"] * len(texts)\n",
    "            \n",
    "            for i, valid_idx in enumerate(valid_indices):\n",
    "                if i < len(keywords_results) and keywords_results[i]:\n",
    "                    # Extraer solo las keywords (sin scores)\n",
    "                    keyword_list = [kw[0] if isinstance(kw, tuple) else kw for kw in keywords_results[i]]\n",
    "                    results[valid_idx] = \", \".join(keyword_list)\n",
    "            \n",
    "            return results\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extrayendo keywords en lote: {e}\")\n",
    "            return [\"\"] * len(texts)\n",
    "    \n",
    "    def process_emails(self, batch_size=8, save_every=10):\n",
    "        \"\"\"Procesar emails con sistema de checkpoint usando lotes verdaderos\"\"\"\n",
    "        \n",
    "        # Cargar checkpoint\n",
    "        processed_indices = self.load_checkpoint()\n",
    "        \n",
    "        # Filtrar emails no procesados\n",
    "        remaining_df = self.df[~self.df.index.isin(processed_indices)]\n",
    "        \n",
    "        if len(remaining_df) == 0:\n",
    "            logger.info(\"Todos los emails ya han sido procesados\")\n",
    "            return self.df\n",
    "        \n",
    "        logger.info(f\"Procesando {len(remaining_df)} emails restantes de {len(self.df)} totales\")\n",
    "        \n",
    "        # Crear dataset\n",
    "        dataset = EmailDataset(remaining_df)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Procesar con barra de progreso\n",
    "        with tqdm(total=len(dataset), desc=\"Extrayendo keywords\") as pbar:\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                # Preparar lote de textos\n",
    "                batch_texts = []\n",
    "                batch_indices = []\n",
    "                \n",
    "                for i in range(len(batch['index'])):\n",
    "                    idx = batch['index'][i].item()\n",
    "                    body = batch['body'][i]\n",
    "                    subject = batch['subject'][i]\n",
    "                    \n",
    "                    # Combinar subject y body\n",
    "                    full_text = f\"{subject} {body}\".strip()\n",
    "                    batch_texts.append(full_text)\n",
    "                    batch_indices.append(idx)\n",
    "                \n",
    "                try:\n",
    "                    # Procesar lote completo de una vez\n",
    "                    batch_keywords = self.extract_keywords_batch(batch_texts)\n",
    "                    \n",
    "                    # Asignar resultados\n",
    "                    for idx, keywords in zip(batch_indices, batch_keywords):\n",
    "                        self.df.loc[idx, 'keywords'] = keywords\n",
    "                        processed_indices.add(idx)\n",
    "                    \n",
    "                    # Actualizar barra de progreso\n",
    "                    pbar.update(len(batch_indices))\n",
    "                    \n",
    "                    # Calcular estad√≠sticas del lote\n",
    "                    non_empty_keywords = [k for k in batch_keywords if k]\n",
    "                    avg_keywords = np.mean([len(k.split(', ')) for k in non_empty_keywords]) if non_empty_keywords else 0\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        '√öltimo lote': f\"{batch_indices[0]}-{batch_indices[-1]}\",\n",
    "                        'Avg keywords': f\"{avg_keywords:.1f}\",\n",
    "                        'Con keywords': f\"{len(non_empty_keywords)}/{len(batch_keywords)}\"\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error procesando lote {batch_indices}: {e}\")\n",
    "                    # En caso de error, asignar keywords vac√≠as\n",
    "                    for idx in batch_indices:\n",
    "                        self.df.loc[idx, 'keywords'] = \"\"\n",
    "                        processed_indices.add(idx)\n",
    "                    pbar.update(len(batch_indices))\n",
    "                \n",
    "                batch_count += 1\n",
    "                \n",
    "                # Guardar checkpoint cada ciertos batches\n",
    "                if batch_count % save_every == 0:\n",
    "                    self.save_checkpoint(processed_indices)\n",
    "        \n",
    "        # Guardar checkpoint final\n",
    "        self.save_checkpoint(processed_indices)\n",
    "        \n",
    "        logger.info(f\"Procesamiento completado. {len(processed_indices)} emails procesados\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"Limpiar archivos temporales despu√©s del procesamiento exitoso\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "            if os.path.exists(self.results_file):\n",
    "                os.remove(self.results_file)\n",
    "            logger.info(\"Archivos temporales eliminados\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error eliminando archivos temporales: {e}\")\n",
    "\n",
    "# Funci√≥n principal para usar el extractor\n",
    "def extract_keywords_from_dataframe(df, checkpoint_file='keyword_extraction_checkpoint.pkl', \n",
    "                                  results_file='keyword_results_temp.csv',\n",
    "                                  batch_size=8, save_every=10, cleanup_after=True):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para extraer keywords de un dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con columnas 'body' y 'subject'\n",
    "        checkpoint_file: Archivo para guardar progreso\n",
    "        results_file: Archivo temporal para resultados\n",
    "        batch_size: Tama√±o del batch (recomendado 4-8 para GPU)\n",
    "        save_every: Guardar checkpoint cada N batches\n",
    "        cleanup_after: Limpiar archivos temporales al finalizar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con columna 'keywords' a√±adida\n",
    "    \"\"\"\n",
    "    \n",
    "    extractor = KeywordExtractor(df, checkpoint_file, results_file)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    extractor.setup_model()\n",
    "    \n",
    "    # Procesar emails\n",
    "    result_df = extractor.process_emails(batch_size=batch_size, save_every=save_every)\n",
    "    \n",
    "    # Limpiar archivos temporales si se solicita\n",
    "    if cleanup_after:\n",
    "        extractor.cleanup_temp_files()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "if os.path.exists(\"emails_with_keywords.pkl\"):\n",
    "    df = pd.read_pickle(\"emails_with_keywords.pkl\")  # ‚úÖ Reanuda si existe\n",
    "else:\n",
    "    df = pd.read_csv(\"emails.csv\")                   # üÜï Comienza desde cero\n",
    "\n",
    "# Procesar emails en lotes (m√°s eficiente)\n",
    "df_with_keywords = extract_keywords_from_dataframe(\n",
    "    df, \n",
    "    batch_size=8,          # Procesa 8 emails a la vez\n",
    "    save_every=5,          # Guardar cada 5 lotes procesados\n",
    "    cleanup_after=False    # <-- Aqu√≠ poner False para mantener checkpoint y pkl y poder reanudar\n",
    ")\n",
    "\n",
    "# Guardar resultado parcial/final para poder reanudar\n",
    "df_with_keywords.to_pickle('emails_with_keywords.pkl')  # Guarda el progreso en formato pkl\n",
    "\n",
    "# Opcionalmente, si quieres exportar a CSV para revisar\n",
    "df_with_keywords.to_csv('emails_with_keywords.csv', index=False)\n",
    "\n",
    "# Para reanudar despu√©s de una interrupci√≥n, vuelve a ejecutar este script completo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
