{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7161ec15-b9c6-4655-8cac-1eefc3e32728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from keybert) (2.0.1)\n",
      "Requirement already satisfied: rich>=10.4.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from keybert) (14.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from keybert) (1.6.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from keybert) (4.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from rich>=10.4.0->keybert) (4.12.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers>=0.3.8->keybert) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers>=0.3.8->keybert) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
      "Requirement already satisfied: filelock in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: networkx in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n",
      "Requirement already satisfied: sentence-transformers in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: transformers in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: bitsandbytes in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from bitsandbytes) (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ignacio/anaconda3/envs/tfm/lib/python3.9/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade keybert\n",
    "!pip install sentence-transformers\n",
    "!pip install transformers accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164f3756-7c52-4c92-b507-9231a46d6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138cf4f4-5521-4aae-a7be-6d67b0923648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b580703941d8438081e7124cb61c4f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e5644d30f348d29316e958ad9795af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637c2146017c4ff1b15cca52e2651c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d312a2d194a9427b9772848106fd9edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d432978b7cac4614ade80228acf3338e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04240e1d8f9f436f9d5bb8d547e5f625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6ab6979bba4e4b95f63c66f3401d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b6313c0bf498aa2ed1c984f2fd6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1973434a4d3346ca9deb1810a76d0aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532aec8eca8f42ce9c3b16a2bc432e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ad1ea7fdd64c29833c673e27e849cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "generator = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a69802-b09b-42ff-ae88-47e9e18e4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"\"\"\n",
    "<s>[INST]\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else.\n",
    "[/INST] delivery, website, received, couple of days, still not received</s>\n",
    "\"\"\"\n",
    "\n",
    "keyword_prompt = \"\"\"\n",
    "[INST]\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "KeyBERT has generated the following candidate keywords:\n",
    "[CANDIDATES]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document and just one list\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = example_prompt + keyword_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aec94e8-1a4b-432a-beea-cc0e2e699549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM\n",
    "\n",
    "llm = TextGeneration(generator, prompt=prompt_template)\n",
    "kw_model = KeyLLM(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80aca248-c142-41e9-8d37-5136a22e35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love viagra and I want hardsex\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\",\n",
    "    \"I received my package!\",\n",
    "    \"Meta released LLaMA's model weights to the research community under a noncommercial license.\",\n",
    "    \"My name is Juanje and I like dicks\",\n",
    "    \"I love viagra and I want hardsex\",\n",
    "]\n",
    "\n",
    "\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b3d3c4-eebb-48f2-b7d2-d4978acf6f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4553b5985cda49caa2ebc211066cc89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d662808d26496282733d327f9d85ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd13988dac544ae81836b0e25848d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9107df6a2648aab943660f3da21017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae5714781b54895bac213a9b91ceb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed13ae04fbb54d93b9fc34a2e802d4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb5e4cac3e94db5a77ff8bdc6d4e0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da63773b95824f84993f2167ba707fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4626ff0878ab4fe4b08765a05145f5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5403e97baf43bf8a4a9f435aa565db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1784515c9e40a88073cae5024b0ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['delivery', 'website', 'received', 'couple of days', 'still not received'], ['received package', 'received', 'package'], ['meta released llama', 'llama model weights', 'noncommercial license', 'weights research', 'meta', 'model', 'community'], ['juanje', 'like', 'dicks'], ['viagra', 'love', 'want', 'hardsex']]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "kw_model = KeyBERT(llm=llm, model=embedding_model)\n",
    "\n",
    "keywords = kw_model.extract_keywords(documents, keyphrase_ngram_range=(1, 3), threshold=0.9, diversity=0.7, use_mmr=True, top_n=7)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8f2b3c-c29d-44bf-b364-bfe34f92d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc7ac3d-cb6e-4e87-8046-50532d11d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 18:51:29,823 - INFO - Configurando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903e86b6c90748e8a3323a11e3c7b89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "2025-05-26 18:51:43,584 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-05-26 18:51:43,585 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-05-26 18:51:45,722 - INFO - Modelo configurado correctamente\n",
      "2025-05-26 18:51:45,731 - INFO - Procesando 10000 emails restantes de 10000 totales\n",
      "2025-05-26 18:52:01,671 - INFO - Checkpoint guardado: 5 emails procesadosÚltimo procesado=4, Keywords encontradas=7] \n",
      "2025-05-26 18:52:16,053 - INFO - Checkpoint guardado: 10 emails procesadosÚltimo procesado=9, Keywords encontradas=7]\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "2025-05-26 18:52:34,327 - INFO - Checkpoint guardado: 15 emails procesadosÚltimo procesado=14, Keywords encontradas=7]  \n",
      "2025-05-26 18:52:51,057 - INFO - Checkpoint guardado: 20 emails procesadosÚltimo procesado=19, Keywords encontradas=11] \n",
      "2025-05-26 18:53:08,878 - INFO - Checkpoint guardado: 25 emails procesadosÚltimo procesado=24, Keywords encontradas=7]  \n",
      "2025-05-26 18:53:26,895 - INFO - Checkpoint guardado: 30 emails procesados Último procesado=29, Keywords encontradas=23]\n",
      "2025-05-26 18:53:44,153 - INFO - Checkpoint guardado: 35 emails procesados Último procesado=34, Keywords encontradas=16]\n",
      "2025-05-26 18:54:02,933 - INFO - Checkpoint guardado: 40 emails procesados Último procesado=39, Keywords encontradas=8] \n",
      "2025-05-26 18:54:19,128 - INFO - Checkpoint guardado: 45 emails procesados Último procesado=44, Keywords encontradas=17]\n",
      "2025-05-26 18:54:36,148 - INFO - Checkpoint guardado: 50 emails procesadosÚltimo procesado=49, Keywords encontradas=7]  \n",
      "2025-05-26 18:54:55,685 - INFO - Checkpoint guardado: 55 emails procesados Último procesado=54, Keywords encontradas=20]\n",
      "2025-05-26 18:55:17,714 - INFO - Checkpoint guardado: 60 emails procesados Último procesado=59, Keywords encontradas=22]\n",
      "2025-05-26 18:55:34,206 - INFO - Checkpoint guardado: 65 emails procesadosÚltimo procesado=64, Keywords encontradas=7]  \n",
      "2025-05-26 18:55:52,570 - INFO - Checkpoint guardado: 70 emails procesados Último procesado=69, Keywords encontradas=14]\n",
      "2025-05-26 18:56:08,315 - INFO - Checkpoint guardado: 75 emails procesadosÚltimo procesado=74, Keywords encontradas=7]  \n",
      "Extrayendo keywords:   1%|          | 77/10000 [04:30<9:41:02,  3.51s/it, Último procesado=76, Keywords encontradas=7] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 322\u001b[0m\n\u001b[1;32m    319\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memails.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Procesar emails (se puede interrumpir y reanudar)\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m df_with_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Guardar cada 5 emails procesados\u001b[39;49;00m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcleanup_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    327\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Guardar resultado final\u001b[39;00m\n\u001b[1;32m    330\u001b[0m df_with_keywords\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memails_with_keywords.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 308\u001b[0m, in \u001b[0;36mextract_keywords_from_dataframe\u001b[0;34m(df, checkpoint_file, results_file, batch_size, save_every, cleanup_after)\u001b[0m\n\u001b[1;32m    305\u001b[0m extractor\u001b[38;5;241m.\u001b[39msetup_model()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Procesar emails\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_emails\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Limpiar archivos temporales si se solicita\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup_after:\n",
      "Cell \u001b[0;32mIn[2], line 244\u001b[0m, in \u001b[0;36mKeywordExtractor.process_emails\u001b[0;34m(self, batch_size, save_every)\u001b[0m\n\u001b[1;32m    241\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m keywords\n\u001b[1;32m    246\u001b[0m     processed_indices\u001b[38;5;241m.\u001b[39madd(idx)\n",
      "Cell \u001b[0;32mIn[2], line 191\u001b[0m, in \u001b[0;36mKeywordExtractor.extract_keywords_single\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    187\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(char\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m word) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(char\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m word\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))]) \u001b[38;5;66;03m# more specific for codes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[a-z]*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+[a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md]*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28msum\u001b[39m(c\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28msum\u001b[39m(c\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m), text)\n\u001b[0;32m--> 191\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyphrase_ngram_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiversity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_mmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keywords \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keywords) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# Extraer solo las keywords (sin scores)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     keyword_list \u001b[38;5;241m=\u001b[39m [kw[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kw, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m kw \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m keywords[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/keybert/_model.py:274\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[0;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         candidate_keywords \u001b[38;5;241m=\u001b[39m [[keyword[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords] \u001b[38;5;28;01mfor\u001b[39;00m keywords \u001b[38;5;129;01min\u001b[39;00m all_keywords]\n\u001b[0;32m--> 274\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_keywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_keywords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keywords\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_keywords\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/keybert/_llm.py:108\u001b[0m, in \u001b[0;36mKeyLLM.extract_keywords\u001b[0;34m(self, docs, check_vocab, candidate_keywords, threshold, embeddings)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         selected_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     in_cluster_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     in_cluster_keywords \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    110\u001b[0m         doc_id: in_cluster_keywords[index] \u001b[38;5;28;01mfor\u001b[39;00m index, cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(clusters) \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m cluster\n\u001b[1;32m    111\u001b[0m     }\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Update out cluster keywords with in cluster keywords\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/keybert/llm/_textgeneration.py:120\u001b[0m, in \u001b[0;36mTextGeneration.extract_keywords\u001b[0;34m(self, documents, candidate_keywords)\u001b[0m\n\u001b[1;32m    117\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CANDIDATES]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(candidates))\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Extract result from generator and use that as label\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [keyword\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    122\u001b[0m all_keywords\u001b[38;5;241m.\u001b[39mappend(keywords)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/pipelines/base.py:1379\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         )\n\u001b[1;32m   1377\u001b[0m     )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/pipelines/base.py:1386\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1385\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1386\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/pipelines/base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/generation/utils.py:3437\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m-> 3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3438\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3443\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm/lib/python3.9/site-packages/transformers/generation/utils.py:916\u001b[0m, in \u001b[0;36mGenerationMixin._update_model_kwargs_for_generation\u001b[0;34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[0m\n\u001b[1;32m    910\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    911\u001b[0m             [decoder_attention_mask, decoder_attention_mask\u001b[38;5;241m.\u001b[39mnew_ones((decoder_attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))],\n\u001b[1;32m    912\u001b[0m             dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 916\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_position\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_new_tokens\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     past_positions \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM, KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para cargar emails de forma eficiente\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, start_idx=0):\n",
    "        self.df = dataframe.iloc[start_idx:].reset_index(drop=True)\n",
    "        self.original_indices = dataframe.iloc[start_idx:].index.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'index': self.original_indices[idx],\n",
    "            'body': str(row['body']) if pd.notna(row['body']) else \"\",\n",
    "            'subject': str(row['subject']) if pd.notna(row['subject']) else \"\"\n",
    "        }\n",
    "\n",
    "class KeywordExtractor:\n",
    "    \"\"\"Extractor de keywords con sistema de checkpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, df, checkpoint_file='keyword_extraction_checkpoint.pkl', \n",
    "                 results_file='keyword_results_temp.csv'):\n",
    "        self.df = df.copy()\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.results_file = results_file\n",
    "        self.model = None\n",
    "        self.generator = None\n",
    "        self.kw_model = None\n",
    "        \n",
    "        # Inicializar columna de keywords si no existe\n",
    "        if 'keywords' not in self.df.columns:\n",
    "            self.df['keywords'] = None\n",
    "            \n",
    "    def setup_model(self):\n",
    "        \"\"\"Configurar el modelo y pipeline\"\"\"\n",
    "        logger.info(\"Configurando modelo...\")\n",
    "        \n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            model=self.model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            max_new_tokens=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Configurar prompt template\n",
    "        example_prompt = \"\"\"\n",
    "<s>[INST]\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else.\n",
    "[/INST] delivery, website, received, couple of days, still not received</s>\n",
    "\"\"\"\n",
    "\n",
    "        keyword_prompt = \"\"\"\n",
    "[INST]\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "KeyBERT has generated the following candidate keywords:\n",
    "[CANDIDATES]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document and just one list\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "        prompt_template = example_prompt + keyword_prompt\n",
    "\n",
    "        # Configurar KeyBERT con LLM\n",
    "        llm = TextGeneration(self.generator, prompt=prompt_template)\n",
    "        \n",
    "        # Modelo de embeddings\n",
    "        embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "        \n",
    "        self.kw_model = KeyBERT(llm=llm, model=embedding_model)\n",
    "        \n",
    "        logger.info(\"Modelo configurado correctamente\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Cargar checkpoint si existe\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'rb') as f:\n",
    "                    checkpoint = pickle.load(f)\n",
    "                \n",
    "                processed_indices = checkpoint.get('processed_indices', set())\n",
    "                \n",
    "                # Cargar resultados parciales si existen\n",
    "                if os.path.exists(self.results_file):\n",
    "                    temp_results = pd.read_csv(self.results_file)\n",
    "                    for _, row in temp_results.iterrows():\n",
    "                        if row['index'] in self.df.index:\n",
    "                            self.df.loc[row['index'], 'keywords'] = row['keywords']\n",
    "                \n",
    "                logger.info(f\"Checkpoint cargado: {len(processed_indices)} emails ya procesados\")\n",
    "                return processed_indices\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error cargando checkpoint: {e}\")\n",
    "                return set()\n",
    "        \n",
    "        return set()\n",
    "    \n",
    "    def save_checkpoint(self, processed_indices):\n",
    "        \"\"\"Guardar checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'processed_indices': processed_indices,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_processed': len(processed_indices)\n",
    "            }\n",
    "            \n",
    "            with open(self.checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "            # Guardar resultados parciales\n",
    "            processed_df = self.df[self.df['keywords'].notna()].copy()\n",
    "            processed_df = processed_df.reset_index()\n",
    "            processed_df[['index', 'keywords']].to_csv(self.results_file, index=False)\n",
    "            \n",
    "            logger.info(f\"Checkpoint guardado: {len(processed_indices)} emails procesados\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando checkpoint: {e}\")\n",
    "    \n",
    "    def extract_keywords_single(self, text):\n",
    "        \"\"\"Extraer keywords de un texto individual\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Limpiar texto\n",
    "            text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "            text = ' '.join(text.split())  # Normalizar espacios\n",
    "            # Remover caracteres especiales y números, mantener solo letras y espacios\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "            # Eliminar URLs y direcciones de correo\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            text = re.sub(r'\\S*@\\S*\\s?', '', text) # Eliminar emails\n",
    "        \n",
    "            # Eliminar caracteres especiales y puntuación (dejando solo letras y números por ahora)\n",
    "            text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        \n",
    "            # 5. Eliminar números sueltos y códigos alfanuméricos largos\n",
    "            text = re.sub(r'\\b\\d+\\b', '', text) # Eliminar números sueltos\n",
    "            # Eliminar palabras que son predominantemente números o códigos (ej. >4 chars y tiene un número)\n",
    "            text = \" \".join([word for word in text.split() if not (len(word) > 3 and any(char.isdigit() for char in word) and not any(char.isalpha() for char in word.replace('.','')))]) # more specific for codes\n",
    "            text = re.sub(r'\\b[a-z]*\\d+[a-z\\d]*\\b', lambda m: '' if len(m.group(0)) > 5 and sum(c.isdigit() for c in m.group(0)) > sum(c.isalpha() for c in m.group(0)) / 2 else m.group(0), text)\n",
    "            \n",
    "            \n",
    "            keywords = self.kw_model.extract_keywords(\n",
    "                [text], \n",
    "                keyphrase_ngram_range=(1, 3), \n",
    "                threshold=0.9, \n",
    "                diversity=0.7, \n",
    "                use_mmr=True, \n",
    "                top_n=7\n",
    "            )\n",
    "            \n",
    "            if keywords and len(keywords) > 0:\n",
    "                # Extraer solo las keywords (sin scores)\n",
    "                keyword_list = [kw[0] if isinstance(kw, tuple) else kw for kw in keywords[0]]\n",
    "                return \", \".join(keyword_list)\n",
    "            else:\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extrayendo keywords: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def process_emails(self, batch_size=1, save_every=10):\n",
    "        \"\"\"Procesar emails con sistema de checkpoint\"\"\"\n",
    "        \n",
    "        # Cargar checkpoint\n",
    "        processed_indices = self.load_checkpoint()\n",
    "        \n",
    "        # Filtrar emails no procesados\n",
    "        remaining_df = self.df[~self.df.index.isin(processed_indices)]\n",
    "        \n",
    "        if len(remaining_df) == 0:\n",
    "            logger.info(\"Todos los emails ya han sido procesados\")\n",
    "            return self.df\n",
    "        \n",
    "        logger.info(f\"Procesando {len(remaining_df)} emails restantes de {len(self.df)} totales\")\n",
    "        \n",
    "        # Crear dataset\n",
    "        dataset = EmailDataset(remaining_df)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Procesar con barra de progreso\n",
    "        with tqdm(total=len(dataset), desc=\"Extrayendo keywords\") as pbar:\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                for i in range(len(batch['index'])):\n",
    "                    idx = batch['index'][i].item()\n",
    "                    body = batch['body'][i]\n",
    "                    subject = batch['subject'][i]\n",
    "                    \n",
    "                    # Combinar subject y body\n",
    "                    full_text = f\"{subject} {body}\".strip()\n",
    "                    \n",
    "                    try:\n",
    "                        keywords = self.extract_keywords_single(full_text)\n",
    "                        self.df.loc[idx, 'keywords'] = keywords\n",
    "                        processed_indices.add(idx)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error procesando email {idx}: {e}\")\n",
    "                        self.df.loc[idx, 'keywords'] = \"\"\n",
    "                        processed_indices.add(idx)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({\n",
    "                        'Último procesado': idx,\n",
    "                        'Keywords encontradas': len(keywords.split(', ')) if keywords else 0\n",
    "                    })\n",
    "                \n",
    "                batch_count += 1\n",
    "                \n",
    "                # Guardar checkpoint cada ciertos batches\n",
    "                if batch_count % save_every == 0:\n",
    "                    self.save_checkpoint(processed_indices)\n",
    "        \n",
    "        # Guardar checkpoint final\n",
    "        self.save_checkpoint(processed_indices)\n",
    "        \n",
    "        logger.info(f\"Procesamiento completado. {len(processed_indices)} emails procesados\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"Limpiar archivos temporales después del procesamiento exitoso\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "            if os.path.exists(self.results_file):\n",
    "                os.remove(self.results_file)\n",
    "            logger.info(\"Archivos temporales eliminados\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error eliminando archivos temporales: {e}\")\n",
    "\n",
    "# Función principal para usar el extractor\n",
    "def extract_keywords_from_dataframe(df, checkpoint_file='keyword_extraction_checkpoint.pkl', \n",
    "                                  results_file='keyword_results_temp.csv',\n",
    "                                  batch_size=1, save_every=10, cleanup_after=True):\n",
    "    \"\"\"\n",
    "    Función principal para extraer keywords de un dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con columnas 'body' y 'subject'\n",
    "        checkpoint_file: Archivo para guardar progreso\n",
    "        results_file: Archivo temporal para resultados\n",
    "        batch_size: Tamaño del batch (recomendado 1 para este modelo)\n",
    "        save_every: Guardar checkpoint cada N batches\n",
    "        cleanup_after: Limpiar archivos temporales al finalizar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con columna 'keywords' añadida\n",
    "    \"\"\"\n",
    "    \n",
    "    extractor = KeywordExtractor(df, checkpoint_file, results_file)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    extractor.setup_model()\n",
    "    \n",
    "    # Procesar emails\n",
    "    result_df = extractor.process_emails(batch_size=batch_size, save_every=save_every)\n",
    "    \n",
    "    # Limpiar archivos temporales si se solicita\n",
    "    if cleanup_after:\n",
    "        extractor.cleanup_temp_files()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "# Cargar tu dataframe\n",
    "df = pd.read_csv('emails.csv')\n",
    "\n",
    "# Procesar emails (se puede interrumpir y reanudar)\n",
    "df_with_keywords = extract_keywords_from_dataframe(\n",
    "    df, \n",
    "    batch_size=1,\n",
    "    save_every=5,  # Guardar cada 5 emails procesados\n",
    "    cleanup_after=True\n",
    ")\n",
    "\n",
    "# Guardar resultado final\n",
    "df_with_keywords.to_csv('emails_with_keywords.csv', index=False)\n",
    "\n",
    "# Para reanudar después de una interrupción, simplemente ejecuta de nuevo:\n",
    "# df_with_keywords = extract_keywords_from_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23213511-c7b5-40e0-960d-784f28eed2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 19:15:19,095 - INFO - Configurando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df53ca549d944a0beb19b7ad45b2138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use cuda:0\n",
      "2025-05-26 19:15:26,169 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-05-26 19:15:26,170 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-05-26 19:15:28,016 - INFO - Modelo configurado correctamente\n",
      "2025-05-26 19:15:28,022 - INFO - Procesando 10000 emails restantes de 10000 totales\n",
      "Extrayendo keywords:   0%|          | 24/10000 [01:16<8:50:40,  3.19s/it, Último lote=16-23, Avg keywords=14.1, Con keywords=8/8]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM, KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para cargar emails de forma eficiente\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, start_idx=0):\n",
    "        self.df = dataframe.iloc[start_idx:].reset_index(drop=True)\n",
    "        self.original_indices = dataframe.iloc[start_idx:].index.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'index': self.original_indices[idx],\n",
    "            'body': str(row['body']) if pd.notna(row['body']) else \"\",\n",
    "            'subject': str(row['subject']) if pd.notna(row['subject']) else \"\"\n",
    "        }\n",
    "\n",
    "class KeywordExtractor:\n",
    "    \"\"\"Extractor de keywords con sistema de checkpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, df, checkpoint_file='keyword_extraction_checkpoint.pkl', \n",
    "                 results_file='keyword_results_temp.csv'):\n",
    "        self.df = df.copy()\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.results_file = results_file\n",
    "        self.model = None\n",
    "        self.generator = None\n",
    "        self.kw_model = None\n",
    "        \n",
    "        # Inicializar columna de keywords si no existe\n",
    "        if 'keywords' not in self.df.columns:\n",
    "            self.df['keywords'] = None\n",
    "            \n",
    "    def setup_model(self):\n",
    "        \"\"\"Configurar el modelo y pipeline\"\"\"\n",
    "        logger.info(\"Configurando modelo...\")\n",
    "        \n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            model=self.model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            max_new_tokens=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Configurar prompt template\n",
    "        example_prompt = \"\"\"\n",
    "<s>[INST]\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else.\n",
    "[/INST] delivery, website, received, couple of days, still not received</s>\n",
    "\"\"\"\n",
    "\n",
    "        keyword_prompt = \"\"\"\n",
    "[INST]\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "KeyBERT has generated the following candidate keywords:\n",
    "[CANDIDATES]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document and just one list\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "        prompt_template = example_prompt + keyword_prompt\n",
    "\n",
    "        # Configurar KeyBERT con LLM\n",
    "        llm = TextGeneration(self.generator, prompt=prompt_template)\n",
    "        \n",
    "        # Modelo de embeddings\n",
    "        embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "        \n",
    "        self.kw_model = KeyBERT(llm=llm, model=embedding_model)\n",
    "        \n",
    "        logger.info(\"Modelo configurado correctamente\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Cargar checkpoint si existe\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'rb') as f:\n",
    "                    checkpoint = pickle.load(f)\n",
    "                \n",
    "                processed_indices = checkpoint.get('processed_indices', set())\n",
    "                \n",
    "                # Cargar resultados parciales si existen\n",
    "                if os.path.exists(self.results_file):\n",
    "                    temp_results = pd.read_csv(self.results_file)\n",
    "                    for _, row in temp_results.iterrows():\n",
    "                        if row['index'] in self.df.index:\n",
    "                            self.df.loc[row['index'], 'keywords'] = row['keywords']\n",
    "                \n",
    "                logger.info(f\"Checkpoint cargado: {len(processed_indices)} emails ya procesados\")\n",
    "                return processed_indices\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error cargando checkpoint: {e}\")\n",
    "                return set()\n",
    "        \n",
    "        return set()\n",
    "    \n",
    "    def save_checkpoint(self, processed_indices):\n",
    "        \"\"\"Guardar checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'processed_indices': processed_indices,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_processed': len(processed_indices)\n",
    "            }\n",
    "            \n",
    "            with open(self.checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "            # Guardar resultados parciales\n",
    "            processed_df = self.df[self.df['keywords'].notna()].copy()\n",
    "            processed_df = processed_df.reset_index()\n",
    "            processed_df[['index', 'keywords']].to_csv(self.results_file, index=False)\n",
    "            \n",
    "            logger.info(f\"Checkpoint guardado: {len(processed_indices)} emails procesados\")\n",
    "            self.df.to_pickle(\"emails_with_keywords.pkl\")\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando checkpoint: {e}\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Limpiar y normalizar texto\"\"\"\n",
    "\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())  # Normalizar espacios\n",
    "        # Remover caracteres especiales y números, mantener solo letras y espacios\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "        # Eliminar URLs y direcciones de correo\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S*@\\S*\\s?', '', text) # Eliminar emails\n",
    "        re.sub(r'https?://\\S+', ' ', text) # Por si queda algún http o https suelto\n",
    "    \n",
    "        # Eliminar caracteres especiales y puntuación (dejando solo letras y números por ahora)\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "        # 5. Eliminar números sueltos y códigos alfanuméricos largos\n",
    "        text = re.sub(r'\\b\\d+\\b', '', text) # Eliminar números sueltos\n",
    "        # Eliminar palabras que son predominantemente números o códigos (ej. >4 chars y tiene un número)\n",
    "        text = \" \".join([word for word in text.split() if not (len(word) > 3 and any(char.isdigit() for char in word) and not any(char.isalpha() for char in word.replace('.','')))]) # more specific for codes\n",
    "        text = re.sub(r'\\b[a-z]*\\d+[a-z\\d]*\\b', lambda m: '' if len(m.group(0)) > 5 and sum(c.isdigit() for c in m.group(0)) > sum(c.isalpha() for c in m.group(0)) / 2 else m.group(0), text)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def extract_keywords_batch(self, texts):\n",
    "        \"\"\"Extraer keywords de un lote de textos\"\"\"\n",
    "        if not texts:\n",
    "            return [\"\"] * len(texts)\n",
    "        \n",
    "        try:\n",
    "            # Filtrar textos válidos\n",
    "            valid_texts = []\n",
    "            valid_indices = []\n",
    "            \n",
    "            for i, text in enumerate(texts):\n",
    "                cleaned = self.clean_text(text)\n",
    "                if cleaned:\n",
    "                    valid_texts.append(cleaned)\n",
    "                    valid_indices.append(i)\n",
    "            \n",
    "            if not valid_texts:\n",
    "                return [\"\"] * len(texts)\n",
    "            \n",
    "            # Extraer keywords en lote\n",
    "            keywords_results = self.kw_model.extract_keywords(\n",
    "                valid_texts, \n",
    "                keyphrase_ngram_range=(1, 3), \n",
    "                threshold=0.9, \n",
    "                diversity=0.7, \n",
    "                use_mmr=True, \n",
    "                top_n=5\n",
    "            )\n",
    "            \n",
    "            # Procesar resultados\n",
    "            results = [\"\"] * len(texts)\n",
    "            \n",
    "            for i, valid_idx in enumerate(valid_indices):\n",
    "                if i < len(keywords_results) and keywords_results[i]:\n",
    "                    # Extraer solo las keywords (sin scores)\n",
    "                    keyword_list = [kw[0] if isinstance(kw, tuple) else kw for kw in keywords_results[i]]\n",
    "                    results[valid_idx] = \", \".join(keyword_list)\n",
    "            \n",
    "            return results\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extrayendo keywords en lote: {e}\")\n",
    "            return [\"\"] * len(texts)\n",
    "    \n",
    "    def process_emails(self, batch_size=8, save_every=10):\n",
    "        \"\"\"Procesar emails con sistema de checkpoint usando lotes verdaderos\"\"\"\n",
    "        \n",
    "        # Cargar checkpoint\n",
    "        processed_indices = self.load_checkpoint()\n",
    "        \n",
    "        # Filtrar emails no procesados\n",
    "        remaining_df = self.df[~self.df.index.isin(processed_indices)]\n",
    "        \n",
    "        if len(remaining_df) == 0:\n",
    "            logger.info(\"Todos los emails ya han sido procesados\")\n",
    "            return self.df\n",
    "        \n",
    "        logger.info(f\"Procesando {len(remaining_df)} emails restantes de {len(self.df)} totales\")\n",
    "        \n",
    "        # Crear dataset\n",
    "        dataset = EmailDataset(remaining_df)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Procesar con barra de progreso\n",
    "        with tqdm(total=len(dataset), desc=\"Extrayendo keywords\") as pbar:\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                # Preparar lote de textos\n",
    "                batch_texts = []\n",
    "                batch_indices = []\n",
    "                \n",
    "                for i in range(len(batch['index'])):\n",
    "                    idx = batch['index'][i].item()\n",
    "                    body = batch['body'][i]\n",
    "                    subject = batch['subject'][i]\n",
    "                    \n",
    "                    # Combinar subject y body\n",
    "                    full_text = f\"{subject} {body}\".strip()\n",
    "                    batch_texts.append(full_text)\n",
    "                    batch_indices.append(idx)\n",
    "                \n",
    "                try:\n",
    "                    # Procesar lote completo de una vez\n",
    "                    batch_keywords = self.extract_keywords_batch(batch_texts)\n",
    "                    \n",
    "                    # Asignar resultados\n",
    "                    for idx, keywords in zip(batch_indices, batch_keywords):\n",
    "                        self.df.loc[idx, 'keywords'] = keywords\n",
    "                        processed_indices.add(idx)\n",
    "                    \n",
    "                    # Actualizar barra de progreso\n",
    "                    pbar.update(len(batch_indices))\n",
    "                    \n",
    "                    # Calcular estadísticas del lote\n",
    "                    non_empty_keywords = [k for k in batch_keywords if k]\n",
    "                    avg_keywords = np.mean([len(k.split(', ')) for k in non_empty_keywords]) if non_empty_keywords else 0\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        'Último lote': f\"{batch_indices[0]}-{batch_indices[-1]}\",\n",
    "                        'Avg keywords': f\"{avg_keywords:.1f}\",\n",
    "                        'Con keywords': f\"{len(non_empty_keywords)}/{len(batch_keywords)}\"\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error procesando lote {batch_indices}: {e}\")\n",
    "                    # En caso de error, asignar keywords vacías\n",
    "                    for idx in batch_indices:\n",
    "                        self.df.loc[idx, 'keywords'] = \"\"\n",
    "                        processed_indices.add(idx)\n",
    "                    pbar.update(len(batch_indices))\n",
    "                \n",
    "                batch_count += 1\n",
    "                \n",
    "                # Guardar checkpoint cada ciertos batches\n",
    "                if batch_count % save_every == 0:\n",
    "                    self.save_checkpoint(processed_indices)\n",
    "        \n",
    "        # Guardar checkpoint final\n",
    "        self.save_checkpoint(processed_indices)\n",
    "        \n",
    "        logger.info(f\"Procesamiento completado. {len(processed_indices)} emails procesados\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"Limpiar archivos temporales después del procesamiento exitoso\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "            if os.path.exists(self.results_file):\n",
    "                os.remove(self.results_file)\n",
    "            logger.info(\"Archivos temporales eliminados\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error eliminando archivos temporales: {e}\")\n",
    "\n",
    "# Función principal para usar el extractor\n",
    "def extract_keywords_from_dataframe(df, checkpoint_file='keyword_extraction_checkpoint.pkl', \n",
    "                                  results_file='keyword_results_temp.csv',\n",
    "                                  batch_size=8, save_every=10, cleanup_after=True):\n",
    "    \"\"\"\n",
    "    Función principal para extraer keywords de un dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con columnas 'body' y 'subject'\n",
    "        checkpoint_file: Archivo para guardar progreso\n",
    "        results_file: Archivo temporal para resultados\n",
    "        batch_size: Tamaño del batch (recomendado 4-8 para GPU)\n",
    "        save_every: Guardar checkpoint cada N batches\n",
    "        cleanup_after: Limpiar archivos temporales al finalizar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con columna 'keywords' añadida\n",
    "    \"\"\"\n",
    "    \n",
    "    extractor = KeywordExtractor(df, checkpoint_file, results_file)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    extractor.setup_model()\n",
    "    \n",
    "    # Procesar emails\n",
    "    result_df = extractor.process_emails(batch_size=batch_size, save_every=save_every)\n",
    "    \n",
    "    # Limpiar archivos temporales si se solicita\n",
    "    if cleanup_after:\n",
    "        extractor.cleanup_temp_files()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "if os.path.exists(\"emails_with_keywords.pkl\"):\n",
    "    df = pd.read_pickle(\"emails_with_keywords.pkl\")  # ✅ Reanuda si existe\n",
    "else:\n",
    "    df = pd.read_csv(\"emails.csv\")                   # 🆕 Comienza desde cero\n",
    "\n",
    "# Procesar emails en lotes (más eficiente)\n",
    "df_with_keywords = extract_keywords_from_dataframe(\n",
    "    df, \n",
    "    batch_size=8,          # Procesa 8 emails a la vez\n",
    "    save_every=5,          # Guardar cada 5 lotes procesados\n",
    "    cleanup_after=False    # <-- Aquí poner False para mantener checkpoint y pkl y poder reanudar\n",
    ")\n",
    "\n",
    "# Guardar resultado parcial/final para poder reanudar\n",
    "df_with_keywords.to_pickle('emails_with_keywords.pkl')  # Guarda el progreso en formato pkl\n",
    "\n",
    "# Opcionalmente, si quieres exportar a CSV para revisar\n",
    "df_with_keywords.to_csv('emails_with_keywords.csv', index=False)\n",
    "\n",
    "# Para reanudar después de una interrupción, vuelve a ejecutar este script completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735658e-18d1-4858-9902-795164100df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
