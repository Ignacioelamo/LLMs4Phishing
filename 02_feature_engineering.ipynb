{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ignacioelamo/LLMs4Phishing/blob/main/02_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación Previa"
      ],
      "metadata": {
        "id": "ZyaTOBLcHWOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargas"
      ],
      "metadata": {
        "id": "VCSqZdcYHlcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall gensim pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rB-DucbmHZnX",
        "outputId": "f4aab4dd-d658-45ee-9ac1-dfcde40b6b74"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pyLDAvis\n",
            "  Using cached pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting joblib>=1.2.0 (from pyLDAvis)\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jinja2 (from pyLDAvis)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting numexpr (from pyLDAvis)\n",
            "  Using cached numexpr-2.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Using cached funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting scikit-learn>=1.0.0 (from pyLDAvis)\n",
            "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting setuptools (from pyLDAvis)\n",
            "  Using cached setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0.0->pyLDAvis)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->pyLDAvis)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached numexpr-2.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (398 kB)\n",
            "Using cached setuptools-80.8.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, funcy, wrapt, tzdata, threadpoolctl, six, setuptools, numpy, MarkupSafe, joblib, smart-open, scipy, python-dateutil, numexpr, jinja2, scikit-learn, pandas, gensim, pyLDAvis\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: funcy\n",
            "    Found existing installation: funcy 2.0\n",
            "    Uninstalling funcy-2.0:\n",
            "      Successfully uninstalled funcy-2.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.8.0\n",
            "    Uninstalling setuptools-80.8.0:\n",
            "      Successfully uninstalled setuptools-80.8.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.1\n",
            "    Uninstalling joblib-1.5.1:\n",
            "      Successfully uninstalled joblib-1.5.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: numexpr\n",
            "    Found existing installation: numexpr 2.10.2\n",
            "    Uninstalling numexpr-2.10.2:\n",
            "      Successfully uninstalled numexpr-2.10.2\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "  Attempting uninstall: pyLDAvis\n",
            "    Found existing installation: pyLDAvis 3.4.1\n",
            "    Uninstalling pyLDAvis-3.4.1:\n",
            "      Successfully uninstalled pyLDAvis-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 funcy-2.0 gensim-4.3.3 jinja2-3.1.6 joblib-1.5.1 numexpr-2.10.2 numpy-1.26.4 pandas-2.2.3 pyLDAvis-3.4.1 python-dateutil-2.9.0.post0 pytz-2025.2 scikit-learn-1.6.1 scipy-1.13.1 setuptools-80.8.0 six-1.17.0 smart-open-7.1.0 threadpoolctl-3.6.0 tzdata-2025.2 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "dateutil",
                  "six"
                ]
              },
              "id": "cdd53d1bc41446f3bc410151cfa8cdc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skGVlwHBIH9q",
        "outputId": "8f266dc4-0f39-4bff-a1e4-ef6676c6f716"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NOMBRE_ARCHIVO = 'emails.csv'\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Ignacioelamo/LLMs4Phishing/main/data/01_combined_cleaned_email_data.csv -O $NOMBRE_ARCHIVO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXsJqVlSjk4N",
        "outputId": "53a43f45-848a-4480-9e76-93baa353dee6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 13:11:26--  https://raw.githubusercontent.com/Ignacioelamo/LLMs4Phishing/main/data/01_combined_cleaned_email_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10064045 (9.6M) [text/plain]\n",
            "Saving to: ‘emails.csv’\n",
            "\n",
            "emails.csv          100%[===================>]   9.60M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-05-23 13:11:27 (151 MB/s) - ‘emails.csv’ saved [10064045/10064045]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías"
      ],
      "metadata": {
        "id": "1Bb8SK06HmzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "fquCcVtNHomi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNCIONES AUXILIARES"
      ],
      "metadata": {
        "id": "xRTwHAOnQ1VI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KEY BERT"
      ],
      "metadata": {
        "id": "JzGYZpm6Q7Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()\n",
        "\n",
        "def extract_keywords_from_text(text):\n",
        "    \"\"\"\n",
        "    Extrae keywords de un texto usando KeyBERT\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.isna(text) or str(text).strip() == '':\n",
        "            return []\n",
        "\n",
        "        keywords = kw_model.extract_keywords(\n",
        "            str(text),\n",
        "            keyphrase_ngram_range=(1, 1),\n",
        "            stop_words='english',\n",
        "            use_mmr=True,\n",
        "            diversity=0.7\n",
        "        )\n",
        "\n",
        "        # Retornar solo las palabras clave (sin los scores)\n",
        "        return [keyword[0] for keyword in keywords]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNGJk7AYQ8qW",
        "outputId": "5895c4b2-965f-4699-e813-7ff1e69587cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2utZUICUZ6C3"
      },
      "source": [
        "# Extracción de las características del cuerpo del correo:\n",
        "1. **body_html=contains_html**: This is a binary feature that represents the presence of HTML in the email body.  \n",
        "2. **body_forms**: This binary feature represents the presence of forms in HTML email bodies.\n",
        "3. **body_noWords**: This feature measures the total number of words occurring in the email.\n",
        "4.  **body_noCharacters**: This feature measures the total number of characters occurring in the email body.\n",
        "5.  **body_noDistinctWords**: This feature measures the total number of distinct words occurring in the body of the email.  \n",
        "6. **body_richness**: The richness is defined as the ratio of the number of words to the number of characters in the document.\n",
        "$$\n",
        "\\text{body_richness} = \\frac{\\text{body_noWords}}{\\text{body_noCharacters}}\n",
        "$$\n",
        "8. **body_noFunctionWords** Chandrasekaran [6] also listed a set of function words that included:  \n",
        "`account`, `access`, `bank`, `credit`, `click`, `identity`, `inconvenience`, `information`, `limited`, `log`, `minutes`, `password`, `recently`, `risk`, `social`, `security`, `service`, and `suspended`.  The `body_noFunctionWords` feature measures the total number of occurrences of these function words in the email body.\n",
        "9. **body_suspension** This binary feature represents the presence of the word **\"suspension\"** in the body of the email.  \n",
        "10.  **body_verifyYourAccount** This binary feature represents the presence of the phrase **\"verify your account\"** in the body of the email.\n",
        "11. **body_text** contains information regarding the context and purpose of an email. For this, we extract the plain text from the email body and use word embedding techniques to represent it.\n",
        "\n",
        "Añadimos las features que ya teníamos: has_attachment, contains_html, urls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3thkqU3Kjgfr"
      },
      "outputs": [],
      "source": [
        "#Es necesario la version de numpy= 1.26.4\n",
        "#%pip install --upgrade --force-reinstall numpy==1.26.4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aqwO-CXHb5w3"
      },
      "outputs": [],
      "source": [
        "df = (\n",
        "    pd.read_csv(NOMBRE_ARCHIVO)\n",
        "      .assign(\n",
        "          # Feature 3: body_noWords (Total number of words)\n",
        "          body_noWords=lambda df: df['body'].apply(lambda x: len(str(x).split())),\n",
        "\n",
        "          # Feature 4: body_noCharacters (Total number of characters)\n",
        "          body_noCharacters=lambda df: df['body'].apply(lambda x: len(str(x))),\n",
        "\n",
        "          # Feature 5: body_noDistinctWords (Total number of distinct words)\n",
        "          body_noDistinctWords=lambda df: df['body'].apply(lambda x: len(set(str(x).split()))),\n",
        "\n",
        "          # Feature 6: body_richness (Ratio of words to characters)\n",
        "          body_richness=lambda df: df['body'].apply(lambda x: len(str(x).split())) / df['body'].apply(lambda x: len(str(x))),\n",
        "\n",
        "          # Feature 7: body_noFunctionWords (Count of specific function words)\n",
        "          body_noFunctionWords=lambda df: df['body'].apply(\n",
        "              lambda x: sum(1 for word in str(x).split()\n",
        "                          if word.lower() in ['account', 'access', 'bank', 'credit', 'click',\n",
        "                                            'identity', 'inconvenience', 'information', 'limited',\n",
        "                                            'log', 'minutes', 'password', 'recently', 'risk',\n",
        "                                            'social', 'security', 'service', 'suspended'])\n",
        "          ),\n",
        "\n",
        "          # Feature 8: body_keywords (Keywords extracted using KeyBERT)\n",
        "          body_keywords=lambda df: df['body'].apply(extract_keywords_from_text)\n",
        "      )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyuzGgVHcoOU",
        "outputId": "311fd256-094c-447a-f0c0-eb10297fd676"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['source', 'subject', 'body', 'contains_html', 'body_forms',\n",
              "       'has_attachment', 'urls', 'label', 'body_noWords', 'body_noCharacters',\n",
              "       'body_noDistinctWords', 'body_richness', 'body_noFunctionWords',\n",
              "       'body_keywords'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['body_keywords'].head(1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx8pJekiTHnf",
        "outputId": "e48d0945-3890-4b5b-ef0d-0d34e90fc78f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0               [viagra, pharmacy, cheapest, send, high]\n",
            "1      [empowerment, 500, 0bligation, returning, real...\n",
            "2                  [watches, vip, gift, stylish, models]\n",
            "3              [watches, real, signature, du4d, cartier]\n",
            "4      [rewards, shopperssavingcenter, kingsbury, dea...\n",
            "                             ...                        \n",
            "995             [speakup, file, braille, types, mailman]\n",
            "996         [unsubscribe, debootstrap, wifi, ata, golov]\n",
            "997                  [websvn, samba, 1092, tagging, bin]\n",
            "998           [profiling, init, ptc, discussed, tickets]\n",
            "999    [debugging, stubs, hierarchy, unsubscribe, ens...\n",
            "Name: body_keywords, Length: 1000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KeyBert"
      ],
      "metadata": {
        "id": "fpFTmbPMIyKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: guarda en una variable todo el texto de la columa body del dataframe\n",
        "\n",
        "email_bodies = df['body'].tolist()\n",
        "all_text = '\\n'.join(email_bodies)"
      ],
      "metadata": {
        "id": "oyyr6wkSJDaZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKpXhV10IhIU",
        "outputId": "2cc69673-b35c-4fc4-84d5-215a8244190b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = kw_model.extract_keywords(all_text, keyphrase_ngram_range=(1, 1), stop_words='english', use_mmr=True, diversity=0.7)\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "EiKAtMigKQUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT8_4V2rct_H"
      },
      "outputs": [],
      "source": [
        "#Usamos el model MiniLM-L6-v2 para generar la feature del body_text\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "df =df.dropna(subset=['body'])\n",
        "emails=df['body'].tolist()\n",
        "embeddings = model.encode(emails, show_progress_bar=True)\n",
        "df[\"body_text\"]=embeddings.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60ZiDdtTiUmR"
      },
      "source": [
        "Para sacar los tópicos de los correos podemos hacerlo de dos formas:\n",
        "1. BERTopic: usa embeddings contextuales para agrupar documentos, y luego re-pondera con TF-IDF para extraer términos.\n",
        "2. LDA: modelo generativo de tópicos sobre Bag-of-Words, que descubre distribuciones de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McVS0uVoj07p"
      },
      "outputs": [],
      "source": [
        "#Lematizamos el body del correo para aplicar el modelo LDA\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T1z5rqHkVAk"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i-PzAl_rFeL"
      },
      "source": [
        "El Coherence Score es una medida de qué tan “coherentes” aparecen los tópicos para la interpretación, basándose en la co-ocurrencia de las palabras más representativas dentro de los documentos. Nos dice cuanto de interpretables son los tópicos. Obtenemos un valor moderado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MkH7RQEkeP6"
      },
      "outputs": [],
      "source": [
        "#MODELO LDA:\n",
        "texts = df['body'].astype(str).tolist()\n",
        "\n",
        "# 2) Prepara stopwords y spaCy para lematizar (igual que en el notebook)\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize_tokens(doc):\n",
        "    parsed = nlp(doc)\n",
        "    return [\n",
        "        token.lemma_.lower()\n",
        "        for token in parsed\n",
        "        if token.lemma_ not in stop_words\n",
        "           and token.is_alpha\n",
        "           and len(token.lemma_) > 3\n",
        "    ]\n",
        "\n",
        "data_lemm = [lemmatize_tokens(doc) for doc in texts]\n",
        "\n",
        "# 3) Detecta bigramas y aplícalos\n",
        "bigram = Phrases(data_lemm, min_count=5, threshold=100)\n",
        "bigram_mod = Phraser(bigram)\n",
        "data_words = [bigram_mod[doc] for doc in data_lemm]\n",
        "\n",
        "# 4) Crea el diccionario y el corpus de Gensim (Bag-of-Words)\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "id2word.filter_extremes(no_below=15, no_above=0.5)\n",
        "corpus = [id2word.doc2bow(text) for text in data_words]\n",
        "\n",
        "# --- Promedio de coherencia sobre varias corridas ---\n",
        "k_values = list(range(10, 16))\n",
        "seeds    = [0, 7, 42, 99, 123]   # distintas semillas\n",
        "results  = {k: [] for k in k_values}\n",
        "\n",
        "for k in k_values:\n",
        "    for seed in seeds:\n",
        "        lda = LdaModel(\n",
        "            corpus=corpus,\n",
        "            id2word=id2word,\n",
        "            num_topics=k,\n",
        "            random_state=seed,\n",
        "            update_every=1,\n",
        "            chunksize=100,\n",
        "            passes=10,\n",
        "            alpha='auto'\n",
        "        )\n",
        "        cm = CoherenceModel(\n",
        "            model=lda,\n",
        "            texts=data_words,\n",
        "            dictionary=id2word,\n",
        "            coherence='c_v'\n",
        "        )\n",
        "        results[k].append(cm.get_coherence())\n",
        "\n",
        "# --- Calcular media y desviación ---\n",
        "means = [np.mean(results[k]) for k in k_values]\n",
        "stds  = [np.std(results[k])  for k in k_values]\n",
        "\n",
        "# --- Graficar con barras de error ---\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.errorbar(k_values, means, yerr=stds, fmt='-o', capsize=5)\n",
        "plt.xticks(k_values)\n",
        "plt.xlabel(\"Número de tópicos (k)\")\n",
        "plt.ylabel(\"Coherence Score (c_v)\")\n",
        "plt.title(\"Coherence vs k (media ± 1σ sobre distintas semillas)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqZjRwmWyMzI"
      },
      "source": [
        "En el k=10 obtenemos el óptimo para el número de tópicos, ya que es el que alcanza el mayor valor medio tiene y la desviación que puede producir al tomar diferentes semillas es baja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXebh2Z0y44m"
      },
      "outputs": [],
      "source": [
        "#Por lo que usamos k=10 para el modelo LDA\n",
        "lda = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=10,\n",
        "    random_state=42,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# 6) Imprime los términos más representativos de cada tópico\n",
        "for idx, topic in lda.print_topics(num_topics=10, num_words=10):\n",
        "    print(f\"Tópico {idx:2d}: {topic}\")\n",
        "\n",
        "#Graficamos los topicos usando pyLDAvis\n",
        "data = gensimvis.prepare(lda, corpus, id2word)\n",
        "pyLDAvis.display(data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}